* ( ) Medium Problems

** ( ) [Longest Common Subsequence]{https://leetcode.com/problems/longest-common-subsequence/description/}

   %TOPICS: Dynamic_Programming, Tabulation%

*** (x) Approach One

    This problem can be solved using the bottom-up approach of dynamic
    programming. We can first start with the subproblems and then build up the
    solution to the main problem. A 2D array of dimensions $m + 1 \times n + 1$
    filled with zeros can be used to store the solutions to the subproblems,
    where `m` and `n` are the lengths of the two strings. The extra row and
    column accounts for the empty string, whose score will be 0.

    @code python
    def longestCommonSubsequence(self, text1: str, text2: str) -> int:
        dp: List[List[int]] = [[0] * (len(text2) + 1) for _ in range(len(text1 ) + 1)]

        for i in range(len(text1) - 1, -1, -1):
            for j in range(len(text2) - 1, -1, -1):
                if text1[i] == text2[j]:
                    dp[i][j] = 1 + dp[i + 1][j + 1]
                else:
                    dp[i][j] = max(dp[i + 1][j], dp[i][j + 1])

        return dp[0][0]
    @end

    *Time Complexity*: $O(m \times n)$
    *Space Complexity*: $O(m \times n)$

** ( ) [Longest Palindromic Subsequence]{https://leetcode.com/problems/longest-palindromic-subsequence/description/}

   %TOPICS: Dynamic_Programming, Memoization, Tabulation%

*** (x) Approach One

    This problem can be solved using the same approach as in [Longest Common
    Subsequence]. If the string and its reverse is supplied to the LCS function
    then it is guranteed that the longest palindromic subsequence will be
    returned.

*** (x) Approach Two (Recursion)

    The problem can be visualized as a decision tree problem where at each
    index (or pair of index) we check if there exists the same characters to
    the left and right of the current index.

    @code python
    def longestPalindromeSubseq(self, s: str) -> int:
        def dfs(i: int, j: int):
            if (i < 0) or (j >= len(s)):
                return 0

            if s[i] == s[j]:
                result = dfs(i - 1, j + 1)
                length = 1 if i == j else 2
                return length + result
            else:
                result = max(dfs(i - 1, j), dfs(i, j + 1))
                return result

        result = 0
        for i in range(len(s)):
            result = max(result, dfs(i, i), dfs(i, i + 1))

        return result
    @end

    *Time Complexity*: This has a time complexity of $O(2^n)$.

*** (+) Approach Three (Memoization)

    The time complexity of the previous approach can be improved by caching the
    results of the subproblems.

    @code python
    def longestPalindromeSubseq(self, s: str) -> int:
        cache = {}
        def dfs(i: int, j: int):
            if (i < 0) or (j >= len(s)):
                return 0
            # return if result exists in cache
            if (i, j) in cache:
                return cache[(i, j)]

            if s[i] == s[j]:
                result = dfs(i - 1, j + 1)
                length = 1 if i == j else 2
                cache[(i, j)] = length + result # cache the result
            else:
                result = max(dfs(i - 1, j), dfs(i, j + 1))
                cache[(i, j)] = result # cache thre resutl
            return cache[(i, j)]

        result = 0
        for i in range(len(s)):
            result = max(result, dfs(i, i), dfs(i, i + 1))

        return result
    @end

    *Time Complexity*: This improves the time complexity form $O(2^n)$ to
    $O(n^2)$.

*** (!) Approach Four (Tabulation)

    Rather than building the solutions to subproblems top-down, we can build
    them from the bottom and move up.

    @code python
    def longestPalindromeSubseq(self, s: str) -> int:
        dp = [[0] * (len(s) + 1) for _ in range(len(s) + 1)]
        result = 0
        for i in range(len(s)):
            for j in range(len(s) - 1, i - 1, -1):
                if s[i] == s[j]:
                    dp[i][j] = 1 if i == j else 2
                    if i - 1 >= 0:
                        dp[i][j] += dp[i - 1][j + 1]
                else:
                    dp[i][j] = dp[i][j + 1]
                    if i - 1 >= 0:
                        dp[i][j] = max(dp[i][j], dp[i - 1][j])
                result = max(result, dp[i][j])

        for arr in dp:
            print(arr, "\n")
        return result

    # dp array after dry-run
    # [1, 2, 2, 2, 2, 0]
    # [0, 3, 4, 2, 2, 0]
    # [0, 0, 3, 2, 2, 0]
    # [0, 0, 0, 3, 2, 0]
    # [0, 0, 0, 0, 1, 0]
    # [0, 0, 0, 0, 0, 0]
    @end

    *Time Complexity*: The outer loop runs from `i = 0` to `len(s) - 1` and for
    each `i`, the inner loop iterates from `j = len(s) - 1` to `i`.

    In the worst case, there will be: $n(n+1)/2$ iterations, where `n`
    is `len(s)`. Hence the overall time complexity is $O(n^2)$.

    *Space Complexity*: The size of the `dp` array is `len(s) + 1` by `len(s) +
    1`, which requires $O(n^2)$ space.

** (x) [House Robber]{https://leetcode.com/problems/house-robber/description/}

   %TOPICS: Dynamic_Programming, Memoization, Tabulation%

*** (x) Approach One

    The problem can be visualized just like a decision problem; At each step we
    can rob any house except the house adjacent to the house we are currently
    at. It makes sense to rob any previous house of the same parity to the
    house with relatively more money.

    Example: For houses `[1, 2, 1, 1, 4, 10]`, if we want to rob the last house,
    we may also rob the house at index 3 and index 1, to maximise the robbery.

    We can spawn a recursive function at each index `i + 2 .. len(n) - 1`.

    @code python
    def rob(self, houses: List[int]) -> int:
        n = len(houses)
        def r(idx: int) -> int:
            if idx >= n:
                return 0
            inter_result = 0
            for i in range(idx + 2, n):
                inter_result = max(inter_result, r(i))
            result = inter_result + houses[idx]
            return result
        return max(r(0), r(1))
    @end

    The recurrence relation for the program can be expressed as:

    @math
    \[
    T(\text{idx}) = \sum_{i=\text{idx}+2}^{n-1} T(i) + O(n-\text{idx})
    \]
    \[
    T(\text{idx}) = \sum_{i=\text{idx}+2}^{n-1} T(i) + O(n)
    \]
    @end

    *Time Complexity*: This gives us exponential time complexity of
    approximately $O(2^n)$.

    *Space Complexity*: Due to the for-loop in the recursive function, the
    number of function invocations will be exponential (roughly
    $O(2^n)$), in the worst case. Whereas the number of active calls on
    the stack at any time will be $O(n)$.

*** (x) Approach Two (Memoization)

    @code python
    def rob(self, houses: List[int]) -> int:
        n = len(houses)
        cache = {}

        def r(idx: int) -> int:
            if idx >= n:
                return 0
            if idx in cache:
                return cache[idx]
            inter_result = 0
            for i in range(idx + 2, n):
                inter_result = max(inter_result, r(i))
            result = inter_result + houses[idx]
            cache[idx] = result
            return result

        return max(r(0), r(1))
    @end

    This program iterates from index `idx + 2 .. n - 1` at each recursive
    call.

    *Time Complexity*: Since each index is processed once, and the processing
    of an index takes $O(n)$ time (due to loop), the overall time complexity of
    the program is approximately $O(n^2)$.

    *Space Complexity*: The space complexity is still $O(n)$ due to the `cache`
    and the recursive call stack having $O(n)$ active calls at any time.

*** (x) Approach Three (Optimized Memoization)

    We don't need to iterate over all the indices from `idx + 2` to `n - 1`.
    The problem can be thought of as mainly having two options when calculating
    result for each index; whether to rob or not rob the current house.

    - If we rob the current house, we can not rob the next house, but can rob
      subsequent houses.
    - If we do not rob the current house, we can rob the next house.

    @code python
    def rob(self, houses: List[int]) -> int:
        n = len(houses)
        cache = {}

        def r(idx: int) -> int:
            if idx >= n:
                return 0
            if idx in cache:
                return cache[idx]
            result = max((houses[idx] + r(idx + 2)), r(idx + 1))
            cache[idx] = result
            return result

        return r(0)
    @end

    Here's the recurrence relation for the above program:

    @math
    \[
    r(i) =
    \begin{cases}
    \max\Bigl(\text{houses}[i] + r(i+2),\; r(i+1) \Bigr), & 0 \le i < n, \\
    0, & i \ge n.
    \end{cases}
    \]
    @end

    *Time Complexity*: Due to caching the result for any index is calculated at
    most once, hence the time complexity will be $O(n)$.

    *Space Complexity*: The space complexity will be $O(n)$ due to `cache` and
    the recursive call stack.

*** (x) Approach Four (Tabulation)

    This problem can be solved by iteratively tabulating the previously
    computed results. Build from the ground up using the already known base
    cases.

    @code python
    def rob(self, houses: List[int]) -> int:
        n = len(houses)

        dp = [0] * (n + 2)
        for i in range(n - 1, -1, -1):
            dp[i] = max(dp[i + 1], dp[i + 2] + houses[i])

        return dp[0]
    @end

    Here's the recurrence relation for the above program:

    @math
    \[
    dp(i) =
    \begin{cases}
    \max\Bigl(\text{dp(i+1)},\, dp(i+2) + \text{houses}[i]\Bigr), & 0 \le i < n, \\
    0, & i \ge n.
    \end{cases}
    \]
    @end

    *Time Complexity*: The time complexity is $O(n)$.
    *Space Complexity*: The space complexity is $O(n)$, due to the `dp`
    array.

*** (x) Approach Five (Optimized Tabulation)

    The previous program can be space obtimezed by only keeping two already
    computed variable. These two variables can be used to compute the current
    iteration result.

    @code python
    def rob(self, houses: List[int]) -> int:
        n = len(houses)

        next1 = 0
        next2 = 0
        result = 0
        for i in range(n - 1, -1, -1):
            result = max(next1, next2 + houses[i])
            next2 = next1
            next1 = result
        return result
    @end

    *Time Complexity*: The time complexity is $O(n)$.
    *Space Complexity*: The space complexity is $O(1)$.

** ( ) [Target Sum]{https://leetcode.com/problems/target-sum/description/}

   %TOPICS: Dynamic_Programming, Memoization, Backtracking%

*** (x) Approach One

    The could simply be simulated can converted into a decision problem. We
    take two decision each time. Either we add the number at the current index
    or subtract it.

    @code python
    def findTargetSumWays(self, nums: List[int], target: int) -> int:
        def findWays(idx: int, curr_sum: int) -> int:
            if idx == len(nums):
                return 1 if curr_sum == target else 0

            result1 = findWays(idx + 1, curr_sum + nums[idx])

            result2 = findWays(idx + 1, curr_sum - nums[idx])

            return result1 + result2

        result = findWays(0, 0)
        return result
    @end

    *Time Complexity*: This has an inefficient time complexity of $O(2^n)$.
    *Space Complexity*: At any moment the stack would have at most `n` calls,
    hence space complexity is $O(n)$

*** (x) Approach Two (Memoization)

    The results to the unique states of `(idx, curr_sum)` can be cached into a
    dictionary, avoiding the $2^n$ explosion of recursive calls.

    @code python
    def findTargetSumWays(self, nums: List[int], target: int) -> int:
        cache = {}

        def findWays(idx: int, curr_sum: int) -> int:
            if idx == len(nums):
                return 1 if curr_sum == target else 0

            if (idx, curr_sum) in cache:
                return cache[(idx, curr_sum)]

            result1 = findWays(idx + 1, curr_sum + nums[idx])

            result2 = findWays(idx + 1, curr_sum - nums[idx])

            cache[(idx, curr_sum)] = result1 + result2

            return result1 + result2

        result = findWays(0, 0)
        return result
    @end

    *Time Complexity*: This memoized solution avoids the explosion of $2^n$ by
    caching the results for unique states of `(idx, curr_sum)` The number of
    states is determined by:

    - The index `idx`, which can be from `0` to `n`, so there are $O(n)$ possibilities.
    - The cumulative sum `curr_sum`, which, in the worst-case, can range from
      $-S$ to $+S$, where $S$ is the sum of the absolute values of the numbers in
      `nums`. This gives approximately $2 \times S+1$ possible values.

    @math
    \[
    O(n \times (2S+1)) \approx O(n \times S)
    \]
    @end

    Thus the overall time complexity is $\mathbf{O(n \times S)}$. This is a
    pseudo-polynomial time complexity because it depends on the numerical value
    $S$ rather than just the size of `n` of the input.

    *Space Complexity*: The max recursion depth is still $O(n)$, but the
    memoization dictionary can cache results upto $O(n \times S)$.
